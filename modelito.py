# -*- coding: utf-8 -*-
"""modelito.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E8lQfBn43yPJ-EXSJKkdkZPn-OLTDWDU

# LANGUAGE MODEL

## 1.1 prepare data set
"""

#pip install torch torchvision torchaudio requests

device = torch.device("cpu")

# Load only the first 1000 characters for testing
with open("shakespeare.txt", "r") as file:
    data = file.read()[:1000]

import torch  # The core library for tensors and deep learning
import torch.nn as nn  # Contains tools to build models
from torch.utils.data import Dataset, DataLoader  # Helps us create datasets and batch loaders
import requests  # Used to download the dataset

#DOWNLOAD Shakespeare dataset
#LOAD text into memory

url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
data = requests.get(url)  # Fetch the dataset from the URL, since it is online we need
print("Dataset downloaded and saved as 'shakespeare.txt'")

with open("shakespeare.txt", "w") as file:
    file.write(data.text)

#LOAD text into memory
#PRINT a sample of the text

with open("shakespeare.txt", "r") as file:
    text = file.read()  # Load the entire text into a single string

#first 500 characters to understand the data
print("Sample text:\n", text[:500])

#unique characters (the vocabulary)
chars = sorted(list(set(text)))  #sorted list of unique characters in the text
print(f"Unique characters: {chars}")
print(f"Number of unique characters: {len(chars)}")

#mappings from characters to indices (stoi) and indices to characters (itos)
stoi = {ch: i for i, ch in enumerate(chars)}  #string-to-index mapping
itos = {i: ch for ch, i in stoi.items()}  #index-to-string mapping

#transform the entire text into a numerical format for processing
encoded_text = [stoi[ch] for ch in text]  #replace each character with its index
print(f"Encoded text sample: {encoded_text[:50]}")  #the first 50 encoded numbers

"""## 1.2 personalize dataset"""

#CREATE a class for the dataset
#DEFINE:
 #   - __len__: Return dataset size
 #   - __getitem__: Return (x, y) pairs

class CharDataset(Dataset):
    """
    Custom dataset for character-level language modeling.
    """
    def __init__(self, text, block_size):
        """
        Initialize the dataset.
        :param text: The encoded numerical representation of the text.
        :param block_size: The length of input sequences (e.g., 128).
        """
        self.text = text  # Encoded text as a list of integers
        self.block_size = block_size  # Length of input and target sequences

    def __len__(self):
        """
        Returns the total number of samples in the dataset.
        """
        return len(self.text) - self.block_size

    def __getitem__(self, idx):
        """
        Retrieves a single sample (x, y) at the given index.
        :param idx: The starting index of the sequence.
        """
        # Input sequence (x): block_size characters starting at idx
        x = torch.tensor(self.text[idx: idx + self.block_size], dtype=torch.long)

        # Target sequence (y): block_size characters shifted by one
        y = torch.tensor(self.text[idx + 1: idx + 1 + self.block_size], dtype=torch.long)

        return x, y

# - `text`: Numerical data (encoded_text).
# - `block_size`: Defines how many characters are in each sequence.
# - `__getitem__`: Slices the data to create input (x) and target (y) sequences.

block_size = 128

dataset = CharDataset(encoded_text, block_size)

print(f"Dataset size: {len(dataset)}")

x, y = dataset[0]
print(f"Input (x): {x}")
print(f"Target (y): {y}")

x_chars = ''.join([itos[i.item()] for i in x])  # Decode input sequence
y_chars = ''.join([itos[i.item()] for i in y])  # Decode target sequence

print(f"Decoded Input: {x_chars}")
print(f"Decoded Target: {y_chars}")

"""## Ç¨ñlk`pjohgc1.3 model"""

class GPTModel(nn.Module):
    """
    Model for language-like. Used for DL subject at UNIGE
    """
    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, block_size, dropout=0.1):
        """
        vocab_size: unique characters (size of the vocabulary).
        embed_dim: embedding vectors.
        num_heads: attention heads in the self-attention mechanism.
        num_layers: transformer blocks.
        block_size: length of the input sequence.
        dropout: dropout rate for regularization.
        """
        super().__init__()

        # 1. token embedding layer: maps character indices to dense vectors
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)

        # 2. positional embedding layer: addds positional information to the embeddings
        self.position_embedding = nn.Embedding(block_size, embed_dim)

        # 3. rtransformer blocks: stack of transformer encoder layers
        #deepen the layers and network "undedrstanding"
        self.transformer_blocks = nn.Sequential(*[
            nn.TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=4 * embed_dim, dropout=dropout)
            for _ in range(num_layers)
        ])

        # 4. final layer normalization: stabilizes training
        self.layer_norm = nn.LayerNorm(embed_dim)

        # 5. output layer: projects the transformer output to the vocabulary size
        self.output_layer = nn.Linear(embed_dim, vocab_size)

        # save parameters
        self.block_size = block_size
        self.embed_dim = embed_dim

    def forward(self, x):
        """
        Defines the forward pass of the model.
        :param x: Input tensor of shape (batch_size, block_size).
        :return: Logits of shape (batch_size, block_size, vocab_size).
        """
        # Get batch size and sequence length
        batch_size, seq_len = x.size()

        # 1. convert indices to dense vectors
        token_embeddings = self.token_embedding(x)  # Shape: (batch_size, seq_len, embed_dim)

        # 2.add positional information
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)  # Shape: (1, seq_len)
        position_embeddings = self.position_embedding(positions)  # Shape: (1, seq_len, embed_dim)

        #oken and positional embeddings
        x = token_embeddings + position_embeddings  # Shape: (batch_size, seq_len, embed_dim)

        # 3.transformer blocks
        x = x.permute(1, 0, 2)  # Transformer expects shape: (seq_len, batch_size, embed_dim)
        x = self.transformer_blocks(x)  # Shape: (seq_len, batch_size, embed_dim)
        x = x.permute(1, 0, 2)  # Back to shape: (batch_size, seq_len, embed_dim)

        # 4 normalization
        x = self.layer_norm(x)  # Shape: (batch_size, seq_len, embed_dim)

        # 5. vocabulary size
        logits = self.output_layer(x)  # Shape: (batch_size, seq_len, vocab_size)

        return logits

vocab_size = len(chars)  # number of unique characters in the dataset
embed_dim = 128  # size of the embedding vectors, efficiency and efectiveness
num_heads = 4  # number of attention heads (idk, they said so)
num_layers = 4  # number of transformer blocks -> character level patterns
block_size = 128  # nput sequence length, memory
dropout = 0.1  # prevent ooverfitting

modelito = GPTModel(vocab_size, embed_dim, num_heads, num_layers, block_size, dropout)

# Print the model structure
print(modelito)

#batch
x, y = dataset[0]  # Get a single (x, y) pair
x = x.unsqueeze(0)  # Add a batch dimension (batch_size=1)

logits = modelito(x)  # Shape: (batch_size=1, block_size=128, vocab_size=65)

# Print the shape of the output
print(f"Logits shape: {logits.shape}")

"""## 1.4 training

SET LOSS FUNCTION
-------------------------------
DEFINE AN OPTIMIZER
-------------------------------
SET number of epochs
FOR each epoch:
    FOR each batch in the dataset:
        - Move data to device (CPU/GPU)
        - Pass input through the model (forward pass)
        - Calculate loss
        - Zero out gradients
        - Backpropagate to compute gradients
        - Update weights using optimizer
    PRINT average loss for the epoch
"""

#loss function
loss = nn.CrossEntropyLoss()
# nn.CrossEntropyLoss`:difference between the predicted probabilities and the true labels (target characters).
#works well for multi-class classification, such as predicting the next character.

#optimizer
learning_rate = 0.0003  #stable enough to prevent divergence
optimizer = torch.optim.Adam(modelito.parameters(), lr=learning_rate)
#learning rate determines how large each step is during optimization.
#controls how much the model updates its weights in response to the gradients computed during backpropagation.
#Adam automatically adapts the learning rate for each parameter.

import time

num_epochs = 3
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  #gpu
model = modelito.to(device)

total_start_time = time.time()

for epoch in range(num_epochs):
    """
    training loop for the dataset, remember num_pochs: how many times is going to be seen during this.

    """
    start_time = time.time()
    model.train()  #training mode
    total_loss = 0  #total loss for this epoch

    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)  #batches
    for x, y in dataloader:
        #takes input and target to the same device as the model
        x, y = x.to(device), y.to(device)

        #forward pass-> predictions
        logits = model(x)  # Shape: (batch_size, block_size, vocab_size)

        # Reshape logits and targets for the loss function
        logits = logits.view(-1, logits.size(-1))  #(batch_size * block_size, vocab_size)
        y = y.view(-1)  #(batch_size * block_size)

        # Calculate the loss
        loss_x = loss(logits, y)

        # Backward pass-> gradients
        optimizer.zero_grad()  #clear the things before
        loss_x.backward()

        #update
        optimizer.step()

        #gather all loss -> accuumulate
        total_loss += loss_x.item()

    epoch_time = time.time() - start_time  # Calculate time for this epoch
    avg_loss = total_loss / len(dataloader)  # Calculate average loss for the epoch


    # End of total training timing
    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Time: {epoch_time:.2f} seconds")

total_time = time.time() - total_start_time
print(f"Total Training Time: {total_time:.2f} seconds")

#model.train: Sets the model to training mode
#dataloader: Provides batches of data for training
#logits.view(-1, logits.size(-1)) ???
#loss.backward() all ttainable parameters and computes it
#optimizer.step(): based on gradients does the update

torch.save(model.state_dict(), "gpt_shakespeare.pth") #save the model to a file in the docs
print("Model saved to 'gpt_shakespeare.pth'")
#saves the model as a dictionary with only updated weights

# Recreate the same model architecture
model = GPTModel(vocab_size, embed_dim, num_heads, num_layers, block_size, dropout)
model.load_state_dict(torch.load("gpt_shakespeare.pth"))
model.eval()  # Set the model to evaluation mode

"""## 1.5 generate txt

DEFINE generat text function
TOKENIZE the seed
FOR desired number of characters:
    - pass current context to the model
    - next character
    - Append the character
RETURN generated text
"""

def generate_text(model, seed, max_length, temperature=1.0):
    """
    Generate text using the trained GPTModel.
    Args:
        model: Trained GPTModel.
        seed: Initial string for generation.
        max_length: Number of characters to generate.
        temperature: Controls randomness in predictions.
    Returns:
        Generated text as a string.
    """
    model.eval()  # Set model to evaluation mode
    device = next(model.parameters()).device  # Ensure compatibility with the model's device

    # Tokenize the seed and handle unknown characters
    context_indices = [stoi.get(ch, stoi[' ']) for ch in seed]  # Replace unknowns with space
    context = torch.tensor(context_indices, dtype=torch.long).unsqueeze(0).to(device)
    generated = seed  # Start with the seed text

    for _ in range(max_length - len(seed)):  # Generate until the desired length
        with torch.no_grad():
            logits = model(context)  # Forward pass

        logits = logits[:, -1, :]  # Get logits for the last character
        logits = logits / temperature  # Scale logits by temperature

        probs = torch.softmax(logits, dim=-1)  # Convert logits to probabilities
        next_idx = torch.multinomial(probs, num_samples=1).item()  # Sample the next character

        # Ensure the sampled index is valid
        if next_idx >= len(itos) or next_idx < 0:
            next_idx = stoi[' ']  # Default to space if invalid

        next_char = itos[next_idx]  # Convert index to character
        generated += next_char  # Append to the generated string

        # Update context with the new character
        context = torch.cat([context, torch.tensor([[next_idx]], device=device)], dim=1)

    return generated

print("stoi mapping:", stoi)
print("itos mapping:", itos)
print("Seed characters:", set(seed_text))

missing_chars = [ch for ch in seed_text if ch not in stoi]
print("Missing characters:", missing_chars)

context_indices = [stoi[ch] for ch in seed]
context_indices = [stoi.get(ch, stoi[' ']) for ch in seed]

vocab_size = len(stoi)
invalid_indices = [idx for idx in context_indices if idx >= vocab_size or idx < 0]
print("Invalid indices:", invalid_indices)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
context = torch.tensor(context_indices, dtype=torch.long).unsqueeze(0).to(device)

seed = "O Romeo, O Romeo"
max_length = 200
temperature = 0.8

generated_text = generate_text(model, seed, max_length, temperature)
print("Generated Text:")
print(generated_text)

print(len(stoi), len(itos))

print("Model vocab size:", model.token_embedding.num_embeddings)
print("stoi size:", len(stoi))

print(model)

print("Vocabulary size in stoi:", len(stoi))
print("Vocabulary size in model (from token embedding):", model.token_embedding.num_embeddings)

#low temperature -> deterministic
low_temp_text = generate_text(model, seed_text, max_length, temperature=0.7)
print("\nLow Temperature (0.7):")
print(low_temp_text)

#high temperature -> creative and risky
high_temp_text = generate_text(model, seed_text, max_length, temperature=1.2)
print("\nHigh Temperature (1.2):")
print(high_temp_text)